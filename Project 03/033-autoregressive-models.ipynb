{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import VimeoVideo\n",
    "from pymongo import MongoClient\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete to the create a client to connect to the MongoDB server, assigns the \"air-quality\" database to db, and assigned the \"nairobi\" connection to nairobi\n",
    "client = MongoClient(host='localhost', port=27017)\n",
    "db = client[\"air-quality\"]\n",
    "nairobi = db[\"nairobi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the wrangle function below so that it returns a Series of the resampled data instead of a DataFram\n",
    "def wrangle(collection):\n",
    "    results = collection.find(\n",
    "        {\"metadata.site\": 29, \"metadata.measurement\": \"P2\"},\n",
    "        projection={\"P2\": 1, \"timestamp\": 1, \"_id\": 0},\n",
    "    )\n",
    "\n",
    "    # Read data into DataFrame\n",
    "    df = pd.DataFrame(list(results)).set_index(\"timestamp\")\n",
    "\n",
    "    # Localize timezone\n",
    "    df.index = df.index.tz_localize(\"UTC\").tz_convert(\"Africa/Nairobi\")\n",
    "\n",
    "    # Remove outliers\n",
    "    df = df[df[\"P2\"] < 500]\n",
    "\n",
    "    # Resample to 1hr window\n",
    "    y = df[\"P2\"].resample(\"1H\").mean().fillna(method='ffill')\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your wrangle function to read the data from the nairobi collection into the Series y\n",
    "y = wrangle(nairobi)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.corr(y.shift(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an ACF plot for the data in y. Be sure to label the x-axis as \"Lag [hours]\" and the y-axis as \"Correlation Coefficient\"\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plot_acf(y, ax=ax)\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an ACF plot for the data in y. Be sure to label the x-axis as \"Lag [hours]\" and the y-axis as \"Correlation Coefficient\"\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plot_acf(y, ax=ax)\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an PACF plot for the data in y. Be sure to label the x-axis as \"Lag [hours]\" and the y-axis as \"Correlation Coefficient\"\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plot_pacf(y, ax=ax)\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split y into training and test sets. The first 95% of the data should be in your training set. The remaining 5% should be in the test set\n",
    "cutoff_test = int(len(y)*.95)\n",
    "\n",
    "y_train = y.iloc[:cutoff_test]\n",
    "y_test = y.iloc[cutoff_test:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the baseline mean absolute error for your model\n",
    "y_train_mean = y_train.mean()\n",
    "y_pred_baseline = [y_train_mean] * len(y_train)\n",
    "mae_baseline = mean_absolute_error(y_train, y_pred_baseline)\n",
    "\n",
    "print(\"Mean P2 Reading:\", round(y_train_mean, 2))\n",
    "print(\"Baseline MAE:\", round(mae_baseline, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nstantiate an AutoReg model and fit it to the training data y_train. Be sure to set the lags argument to 26\n",
    "model = AutoReg(y_train, lags=26).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict() #.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list of training predictions for your model and use them to calculate your training mean absolute error\n",
    "y_pred = model.predict().dropna()\n",
    "training_mae = mean_absolute_error(y_train.iloc[26:], y_pred)\n",
    "print(\"Training MAE:\", training_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_resid = y_train-y_pred\n",
    "y_train_resid=model.resid\n",
    "y_train_resid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a plot of y_train_resid\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "y_train_resid.plot(ylabel=\"Residual Value\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a histogram of y_train_resid\n",
    "y_train_resid.hist()\n",
    "plt.xlabel(\"Residual Value\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.title(\"Distribution of Residuals\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an ACF plot of y_train_resid\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plot_acf(y_train_resid, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the test mean absolute error for your model\n",
    "y_pred_test = model.predict(y_test.index.min(), y_test.index.max())\n",
    "test_mae =mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"Test MAE:\", test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame test_predictions that has two columns: \"y_test\" and \"y_pred\". The first should contain the true values for your test set, and the second should contain your model's predictions. Be sure the index of test_predictions matches the index of y_test\n",
    "df_pred_test = pd.DataFrame(\n",
    "    {\"y_test\": y_test, \"y_pred\": y_pred_test}, index=y_test.index\n",
    ")\n",
    "df_pred_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a time series plot for the values in test_predictions using plotly express. Be sure that the y-axis is properly labeled as \"P2\"\n",
    "fig = px.line(df_pred_test, labels={\"value\": \"P2\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform walk-forward validation for your model for the entire test set y_test. Store your model's predictions in the Series y_pred_wfv\n",
    "\n",
    "y_pred_wfv = pd.Series()\n",
    "history = y_train.copy()\n",
    "for i in range(len(y_test)):\n",
    "    model=AutoReg(history, lags=26).fit()\n",
    "    next_pred=model.forecast()\n",
    "    y_pred_wfv=y_pred_wfv.append(next_pred)\n",
    "    history=history.append(y_test[next_pred.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the test mean absolute error for your model\n",
    "test_mae = mean_absolute_error(y_test, y_pred_wfv)\n",
    "print(\"Test MAE (walk forward validation):\", round(test_mae, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the parameters for your trained model\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the values for y_test and y_pred_wfv into the DataFrame df_pred_test (don't forget the index). Then plot df_pred_test using plotly express\n",
    "df_pred_test=pd.DataFrame(\n",
    "    {\"y_test\":y_test, \"y_pred_wfv\": y_pred_wfv}\n",
    ")\n",
    "fig = px.line(df_pred_test, labels={\"value\":\"PM2.5\"})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3863b2c5477f4dfe9734428f9b5659b9c57b4995a7858c229730ccb9c81899f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
