{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Damage with Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from category_encoders import OrdinalEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "        SELECT distinct(i.building_id) AS b_id,\n",
    "           s.*,\n",
    "           d.damage_grade\n",
    "        FROM id_map AS i\n",
    "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "        WHERE district_id = 4\n",
    "    \"\"\"\n",
    "\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "\n",
    "    # Identify leaky columns\n",
    "    drop_cols = [col for col in df.columns if \"post_eq\" in col]\n",
    "\n",
    "    # Add high-cardinality / redundant column\n",
    "    drop_cols.append(\"building_id\")\n",
    "\n",
    "    # Create binary target column\n",
    "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "\n",
    "    # Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "\n",
    "    # Drop multicollinearity column\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "\n",
    "    # Drop columns\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the wrangle function above to import your data set into the DataFrame df. The path to the SQLite database is \"/home/jovyan/nepal.sqlite\"\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your feature matrix X and target vector y. Your target is \"severe_damage\"\n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. And don't forget to set a random_state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide your training data (X_train and y_train) into training and validation sets using a randomized train-test split. Your validation data should be 20% of the remaining data. Don't forget to set a random_state\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the baseline accuracy score for your model\n",
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline named model that contains a OrdinalEncoder transformer and a DecisionTreeClassifier predictor. (Be sure to set a random_state for your predictor.) Then fit your model to the training data.\n",
    "# Build Model\n",
    "model = make_pipeline(\n",
    "    OrdinalEncoder(), DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "# Fit model to training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training and validation accuracy scores for your models\n",
    "acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "acc_val = model.score(X_val, y_val)\n",
    "\n",
    "print(\"Training Accuracy:\", round(acc_train, 2))\n",
    "print(\"Validation Accuracy:\", round(acc_val, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the get_depth method on the DecisionTreeClassifier in your model to see how deep your tree grew during training.\n",
    "tree_depth = model.named_steps[\"decisiontreeclassifier\"].get_depth()\n",
    "print(\"Tree Depth:\", tree_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a range of possible values for max_depth hyperparameter of your model's DecisionTreeClassifier. depth_hyperparams should range from 1 to 50 by steps of 2\n",
    "depth_hyperparams = range(1,50,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the code below so that it trains a model for every max_depth in depth_hyperparams. Every time a new model is trained, the code should also calculate the training and validation accuracy scores and append them to the training_acc and validation_acc lists, respectively.\n",
    "# Create empty lists for training and validation accuracy scores\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "\n",
    "for d in depth_hyperparams:\n",
    "    # Create model with `max_depth` of `d`\n",
    "    test_model = make_pipeline(\n",
    "        OrdinalEncoder(), \n",
    "        DecisionTreeClassifier(max_depth=d,random_state=42)\n",
    "    )\n",
    "    # Fit model to training data\n",
    "    test_model.fit(X_train, y_train)\n",
    "    # Calculate training accuracy score and append to `training_acc`\n",
    "    training_acc.append(test_model.score(X_train, y_train))\n",
    "    # Calculate validation accuracy score and append to `training_acc`\n",
    "    validation_acc.append(test_model.score(X_val, y_val))\n",
    "\n",
    "print(\"Training Accuracy Scores:\", training_acc[:3])\n",
    "print(\"Validation Accuracy Scores:\", validation_acc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a visualization with two lines. The first line should plot the training_acc values as a function of depth_hyperparams, and the second should plot validation_acc as a function of depth_hyperparams. You x-axis should be labeled \"Max Depth\", and the y-axis \"Accuracy Score\". Also include a legend so that your audience can distinguish between the two lines\n",
    "# Plot `depth_hyperparams`, `training_acc`\n",
    "plt.plot(depth_hyperparams, training_acc, label=\"training\")\n",
    "plt.plot(depth_hyperparams, validation_acc, label=\"validation\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"accuracy score\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on your visualization, choose the max_depth value that leads to the best validation accuracy score. Then retrain your original model with that max_depth value. Lastly, check how your tuned model performs on your test set by calculating the test accuracy score below. Were you able to resolve the overfitting problem with this new max_depth\n",
    "model = make_pipeline(\n",
    "    OrdinalEncoder(), DecisionTreeClassifier(max_depth=6,random_state=42)\n",
    ")\n",
    "# Fit model to training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training and validation accuracy scores for your models\n",
    "acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "acc_val = model.score(X_val, y_val)\n",
    "\n",
    "print(\"Training Accuracy:\", round(acc_train, 2))\n",
    "print(\"Validation Accuracy:\", round(acc_val, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", round(test_acc, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the code below to use the plot_tree function from scikit-learn to visualize the decision logic of your model\n",
    "# Create larger figure\n",
    "fig, ax = plt.subplots(figsize=(25, 12))\n",
    "# Plot tree\n",
    "plot_tree(\n",
    "    decision_tree=model.named_steps[\"decisiontreeclassifier\"],\n",
    "    feature_names= X_train.columns,\n",
    "    filled=True,  # Color leaf with class\n",
    "    rounded=True,  # Round leaf edges\n",
    "    proportion=True,  # Display proportion of classes in leaf\n",
    "    max_depth=3,  # Only display first 3 levels\n",
    "    fontsize=12,  # Enlarge font\n",
    "    ax=ax,  # Place in figure axis\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the feature names and importances of your model to the variables below. For the features, you can get them from the column names in your training set. For the importances, you access the feature_importances_ attribute of your model's DecisionTreeClassifier\n",
    "features = X_train.columns\n",
    "importances = model.named_steps[\"decisiontreeclassifier\"].feature_importances_\n",
    "\n",
    "print(\"Features:\", features[:3])\n",
    "print(\"Importances:\", importances[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pandas Series named feat_imp, where the index is features and the values are your importances. The Series should be sorted from smallest to largest importance.\n",
    "feat_imp = pd.Series(importances, index=features)\n",
    "feat_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a horizontal bar chart with all the features in feat_imp. Be sure to label your x-axis \"Gini Importance\"\n",
    "# Create horizontal bar chart\n",
    "feat_imp.plot(kind=\"barh\")\n",
    "plt.xlabel(\"Gini importance\")\n",
    "plt.ylabel(\"feature\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
