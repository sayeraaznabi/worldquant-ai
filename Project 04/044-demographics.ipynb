{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the Model: Data Ethics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import OneHotEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the cell below to connect to the nepal.sqlite database\n",
    "%load_ext sql\n",
    "%sql sqlite:////home/jovyan/nepal.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select all columns from the household_demographics table, limiting your results to the first five rows.\n",
    "%%sql\n",
    "SELECT *\n",
    "FROM household_demographics\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many observations are in the household_demographics table? Use the count command to find out\n",
    "%%sql\n",
    "SELECT count(*)\n",
    "FROM household_demographics\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select all columns from the id_map table, limiting your results to the first five rows\n",
    "%%sql\n",
    "SELECT *\n",
    "FROM id_map\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a table with all the columns from household_demographics, all the columns from building_structure, the vdcmun_id column from id_map, and the damage_grade column from building_damage. Your results should show only rows where the district_id is 4 and limit your results to the first five rows\n",
    "%%sql\n",
    "SELECT h.*,\n",
    "        s.*,\n",
    "        d.damage_grade,\n",
    "        i.vdcmun_id\n",
    "From household_demographics AS h \n",
    "JOIN id_map AS i ON i.household_id= h.household_id\n",
    "JOIN building_structure AS s ON i.building_id=s.building_id\n",
    "JOIN building_damage AS d ON i.building_id=d.building_id\n",
    "WHERE district_id=4\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"SELECT h.*,\n",
    "        s.*,\n",
    "        d.damage_grade,\n",
    "        i.vdcmun_id\n",
    "From household_demographics AS h \n",
    "JOIN id_map AS i ON i.household_id= h.household_id\n",
    "JOIN building_structure AS s ON i.building_id=s.building_id\n",
    "JOIN building_damage AS d ON i.building_id=d.building_id\n",
    "WHERE district_id=4\"\"\"\n",
    "\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col= \"household_id\")\n",
    "\n",
    "    # Identify leaky columns\n",
    "    drop_cols = [col for col in df.columns if \"post_eq\" in col]\n",
    "\n",
    "    # Add high-cardinality / redundant column\n",
    "    drop_cols.append(\"building_id\")\n",
    "\n",
    "    # Create binary target column\n",
    "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "\n",
    "    # Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "\n",
    "    # Drop multicollinearity column\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "    \n",
    "    # Drop columns\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the select_dtypes and nunique methods to see if there are any high- or low-cardinality categorical features in the dataset\n",
    "# Check for high- and low-cardinality categorical features\n",
    "df.select_dtypes(\"object\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"caste_household\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"caste_household\"].value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"caste_household\"].value_counts().head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10=df[\"caste_household\"].value_counts().head(10).index\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add to your wrangle function so that the \"caste_household\" contains only the 10 largest caste groups. For the rows that are not in those groups, \"caste_household\" should be changed to \"Other\"\n",
    "df[\"caste_household\"].apply(lambda c : c if c in top_10 else \"other\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"SELECT h.*,\n",
    "        s.*,\n",
    "        d.damage_grade,\n",
    "        i.vdcmun_id\n",
    "From household_demographics AS h \n",
    "JOIN id_map AS i ON i.household_id= h.household_id\n",
    "JOIN building_structure AS s ON i.building_id=s.building_id\n",
    "JOIN building_damage AS d ON i.building_id=d.building_id\n",
    "WHERE district_id=4\"\"\"\n",
    "\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col= \"household_id\")\n",
    "\n",
    "    # Identify leaky columns\n",
    "    drop_cols = [col for col in df.columns if \"post_eq\" in col]\n",
    "\n",
    "    # Add high-cardinality / redundant column\n",
    "    drop_cols.append(\"building_id\")\n",
    "\n",
    "    # Create binary target column\n",
    "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "\n",
    "    # Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "\n",
    "    # Drop multicollinearity column\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "    \n",
    "    #Group caste column \n",
    "    top_10=df[\"caste_household\"].value_counts().head(10).index\n",
    "    df[\"caste_household\"]=df[\"caste_household\"].apply(\n",
    "        lambda c : c if c in top_10 else \"other\"\n",
    "    )\n",
    "    \n",
    "    # Drop columns\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your feature matrix X and target vector y. Since our model will only consider building and household data, X should not include the municipality column \"vdcmun_id\". Your target is \"severe_damage\"\n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns = [target, \"vdcmun_id\"])\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. Be sure to set a random_state for reproducibility.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the baseline accuracy score for your model\n",
    "acc_baseline =y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Pipeline called model_lr. It should have an OneHotEncoder transformer and a LogisticRegression predictor. Be sure you set the use_cat_names argument for your transformer to True\n",
    "model_lr = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True), LogisticRegression(max_iter=3000) \n",
    ")\n",
    "model_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the training and test accuracy scores for model_lr\n",
    "acc_train = accuracy_score(y_train, model_lr.predict(X_train))\n",
    "acc_test = model_lr.score(X_test, y_test)\n",
    "\n",
    "print(\"LR Training Accuracy:\", acc_train)\n",
    "print(\"LR Validation Accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, extract the feature names and importances from your model. Then create a pandas Series named feat_imp, where the index is features and the values are your the exponential of the importances\n",
    "features = model_lr.named_steps[\"onehotencoder\"].get_feature_names()\n",
    "importances = model_lr.named_steps[\"logisticregression\"].coef_[0]\n",
    "feat_imp = pd.Series(np.exp(importances), index=features).sort_values()\n",
    "feat_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a horizontal bar chart with the ten largest coefficients from feat_imp. Be sure to label your x-axis \"Odds Ratio\"\n",
    "feat_imp.tail(10).plot(kind=\"barh\")\n",
    "plt.xlabel(\"Odds Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a horizontal bar chart with the ten smallest coefficients from feat_imp. Be sure to label your x-axis \"Odds Ratio\"\n",
    "feat_imp.head(10).plot(kind=\"barh\")\n",
    "plt.xlabel(\"Odds Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which municipalities saw the highest proportion of severely damaged buildings? Create a DataFrame damage_by_vdcmun by grouping df by \"vdcmun_id\" and then calculating the mean of the \"severe_damage\" column. Be sure to sort damage_by_vdcmun from highest to lowest proportion\n",
    "damage_by_vdcmun = (\n",
    "    df.groupby(\"vdcmun_id\")[\"severe_damage\"].mean().sort_values(ascending= False)\n",
    ").to_frame()\n",
    "damage_by_vdcmun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df[df[\"caste_household\"]==\"Gurung\"].groupby(\"vdcmun_id\")[\"severe_damage\"].count()/df.groupby(\"vdcmun_id\")[\"severe_damage\"].count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column in damage_by_vdcmun that contains the the proportion of Gurung households in each municipality\n",
    "damage_by_vdcmun[\"Gurung\"] = (\n",
    "    df[df[\"caste_household\"]==\"Gurung\"].groupby(\"vdcmun_id\")[\"severe_damage\"].count()/df.groupby(\"vdcmun_id\")[\"severe_damage\"].count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column in damage_by_vdcmun that contains the the proportion of Kumal households in each municipality. Replace any NaN values in the column with 0\n",
    "damage_by_vdcmun[\"Kumal\"] =  (\n",
    "    df[df[\"caste_household\"]==\"Kumal\"].groupby(\"vdcmun_id\")[\"severe_damage\"].count()/df.groupby(\"vdcmun_id\")[\"severe_damage\"].count()\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3863b2c5477f4dfe9734428f9b5659b9c57b4995a7858c229730ccb9c81899f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
