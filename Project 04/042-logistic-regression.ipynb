{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Damage with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from category_encoders import OneHotEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "        SELECT distinct(i.building_id) AS b_id,\n",
    "           s.*,\n",
    "           d.damage_grade\n",
    "        FROM id_map AS i\n",
    "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "        WHERE district_id = 4\n",
    "    \"\"\"\n",
    "\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the wrangle function above so that the it returns the results of query as a DataFrame. Be sure that the index column is set to \"b_id\". Also, the path to the SQLite database is \"/home/jovyan/nepal.sqlite\"\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add to your wrangle function so that these features are dropped from the DataFrame. Don't forget to rerun all the cells above\n",
    "drop_cols=[]\n",
    "for col in df.columns:\n",
    "    if \"post_eq\" in col:\n",
    "        drop_cols.append(col)\n",
    "drop_cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=[col for col in df.columns if  \"post_eq\" in col]\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "        SELECT distinct(i.building_id) AS b_id,\n",
    "           s.*,\n",
    "           d.damage_grade\n",
    "        FROM id_map AS i\n",
    "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "        WHERE district_id = 4\n",
    "    \"\"\"\n",
    "\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "    \n",
    "    #identify leaky columns\n",
    "    drop_cols=[col for col in df.columns if  \"post_eq\" in col]\n",
    "    \n",
    "    #drop columns\n",
    "    df.drop(columns=drop_cols , inplace= True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the wrangle function above so that the it returns the results of query as a DataFrame. Be sure that the index column is set to \"b_id\". Also, the path to the SQLite database is \"/home/jovyan/nepal.sqlite\"\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add to your wrangle function so that it creates a new target column \"severe_damage\". For buildings where the \"damage_grade\" is Grade 4 or above, \"severe_damage\" should be 1. For all other buildings, \"severe_damage\" should be 0. Don't forget to drop \"damage_grade\" to avoid leakage, and rerun all the cells above\n",
    "df[\"damage_grade\"]=df[\"damage_grade\"].str[-1].astype(int)\n",
    "df[\"severe_damage\"]=(df[\"damage_grade\"]>3).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "        SELECT distinct(i.building_id) AS b_id,\n",
    "           s.*,\n",
    "           d.damage_grade\n",
    "        FROM id_map AS i\n",
    "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "        WHERE district_id = 4\n",
    "    \"\"\"\n",
    "\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "    \n",
    "    #identify leaky columns\n",
    "    drop_cols=[col for col in df.columns if  \"post_eq\" in col]\n",
    "    \n",
    "    # Create binary target\n",
    "    df[\"damage_grade\"]=df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"]=(df[\"damage_grade\"]>3).astype(int)   \n",
    "    \n",
    "    #Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "    \n",
    "    #drop columns\n",
    "    df.drop(columns=drop_cols , inplace= True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the wrangle function above so that the it returns the results of query as a DataFrame. Be sure that the index column is set to \"b_id\". Also, the path to the SQLite database is \"/home/jovyan/nepal.sqlite\"\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"severe_damage\"].corr(df[\"count_floors_pre_eq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"severe_damage\"].corr(df[\"height_ft_pre_eq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a correlation heatmap of the remaining numerical features in df. Since \"severe_damage\" will be your target, you don't need to include it in your heatmap\n",
    "# Create correlation matrix\n",
    "correlation = df.select_dtypes(\"number\").drop(columns=\"severe_damage\").corr()\n",
    "correlation\n",
    "# Plot heatmap of `correlation`\n",
    "sns.heatmap(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change wrangle function so that it drops the \"count_floors_pre_eq\" column. Don't forget to rerun all the cells above\n",
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "        SELECT distinct(i.building_id) AS b_id,\n",
    "           s.*,\n",
    "           d.damage_grade\n",
    "        FROM id_map AS i\n",
    "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "        WHERE district_id = 4\n",
    "    \"\"\"\n",
    "\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "    \n",
    "    #identify leaky columns\n",
    "    drop_cols=[col for col in df.columns if  \"post_eq\" in col]\n",
    "    \n",
    "    # Create binary target\n",
    "    df[\"damage_grade\"]=df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"]=(df[\"damage_grade\"]>3).astype(int)   \n",
    "    \n",
    "    #Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "    \n",
    "    #Drop multicollinearlity columns\n",
    "    drop_cols,append(\"count_floors_pre_eq\")\n",
    "    \n",
    "    #drop columns\n",
    "    df.drop(columns=drop_cols , inplace= True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the wrangle function above so that the it returns the results of query as a DataFrame. Be sure that the index column is set to \"b_id\". Also, the path to the SQLite database is \"/home/jovyan/nepal.sqlite\"\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use seaborn to create a boxplot that shows the distributions of the \"height_ft_pre_eq\" column for both groups in the \"severe_damage\" column. Remember to label your axes\n",
    "# Create boxplot\n",
    "sns.boxplot(x=\"severe_damage\",y=\"height_ft_pre_eq\", data=df)\n",
    "# Label axes\n",
    "plt.xlabel(\"severe damage\")\n",
    "plt.ylabel(\"height Pre-earthquake [ft.]\")\n",
    "plt.title(\"Distribution of building height by class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bar chart of the value counts for the \"severe_damage\" column. You want to calculate the relative frequencies of the classes, not the raw count, so be sure to set the normalize argument to True\n",
    "# Plot value counts of `\"severe_damage\"`\n",
    "df[\"severe_damage\"].value_counts(normalize=True).plot(kind=\"bar\", xlabel=\"class\", y=\"Relative frequency\", title=\"class balance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two variables, majority_class_prop and minority_class_prop, to store the normalized value counts for the two classes in df[\"severe_damage\"]\n",
    "majority_class_prop, minority_class_prop = df[\"severe_damage\"].value_counts(normalize=True)\n",
    "print(majority_class_prop, minority_class_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are buildings with certain foundation types more likely to suffer severe damage? Create a pivot table of df where the index is \"foundation_type\" and the values come from the \"severe_damage\" column, aggregated by the mean\n",
    "# Create pivot table\n",
    "foundation_pivot = pd.pivot_table(\n",
    "    df, index= \"foundation_type\", values=\"severe_damage\", aggfunc=np.mean).sort_values(by=\"severe_damage\")\n",
    "foundation_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do the proportions in foundation_pivot compare to the proportions for our majority and minority classes? Plot foundation_pivot as horizontal bar chart, adding vertical lines at the values for majority_class_prop and minority_class_prop\n",
    "# Plot bar chart of `foundation_pivot`\n",
    "foundation_pivot.plot(kind=\"barh\", legend=None)\n",
    "plt.axvline(\n",
    "majority_class_prop, linestyle=\"--\", color=\"red\", label=\"majority class\")\n",
    "plt.axvline(\n",
    "minority_class_prop, linestyle=\"--\", color=\"green\", label=\"minority class\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the select_dtypes and nunique methods to see if there are any high- or low-cardinality categorical features in the dataset.\n",
    "# Check for high- and low-cardinality categorical features\n",
    "df.select_dtypes(\"object\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "        SELECT distinct(i.building_id) AS b_id,\n",
    "           s.*,\n",
    "           d.damage_grade\n",
    "        FROM id_map AS i\n",
    "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "        WHERE district_id = 4\n",
    "    \"\"\"\n",
    "\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "    \n",
    "    #identify leaky columns\n",
    "    drop_cols=[col for col in df.columns if  \"post_eq\" in col]\n",
    "    \n",
    "    # Create binary target\n",
    "    df[\"damage_grade\"]=df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"]=(df[\"damage_grade\"]>3).astype(int)   \n",
    "    \n",
    "    #Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "    \n",
    "    #Drop multicollinearlity columns\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "    \n",
    "    #drop high-cardinality catagorical column\n",
    "    drop_cols.append(\"building_id\")\n",
    "    \n",
    "    #drop columns\n",
    "    df.drop(columns=drop_cols , inplace= True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete the wrangle function above so that the it returns the results of query as a DataFrame. Be sure that the index column is set to \"b_id\". Also, the path to the SQLite database is \"/home/jovyan/nepal.sqlite\"\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your feature matrix X and target vector y. Your target is \"severe_damage\"\n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. And don't forget to set a random_state for reproducibility.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size= 0.2, random_state= 42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the baseline accuracy score for your model\n",
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline named model that contains a OneHotEncoder transformer and a LogisticRegression predictor. Be sure you set the use_cat_names argument for your transformer to True. Then fit it to the training data\n",
    "# Build model\n",
    "model = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    LogisticRegression()\n",
    ")\n",
    "# Fit model to training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the training and test accuracy scores for your models\n",
    "acc_train =accuracy_score(y_train, model.predict(X_train))\n",
    "acc_test = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", round(acc_train, 2))\n",
    "print(\"Test Accuracy:\", round(acc_test, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_train)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of using the predict method with your model, try predict_proba with your training data. How does the predict_proba output differ than that of predict? What does it represent?\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:5]\n",
    "print(y_train_pred_proba[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the feature names and importances from your model\n",
    "features = model.named_steps[\"onehotencoder\"].get_feature_names()\n",
    "importances=model.named_steps[\"logisticregression\"].coef_[0]\n",
    "feat_imp = pd.Series(importances, index=features)\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pandas Series named odds_ratios, where the index is features and the values are your the exponential of the importances. How does odds_ratios for this model look different from the other linear models we made in projects 2 and 3\n",
    "odds_ratios = pd.Series(np.exp(importances), index=features).sort_values()\n",
    "odds_ratios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart, five largest coefficients\n",
    "odds_ratios.tail().plot(kind=\"barh\")\n",
    "plt.xlabel(\"Odd Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart, five smallest coefficients\n",
    "odds_ratios.head().plot(kind=\"barh\")\n",
    "plt.xlabel(\"Odd Ratio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3863b2c5477f4dfe9734428f9b5659b9c57b4995a7858c229730ccb9c81899f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
