{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Price with Neighborhood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wqet_grader\n",
    "from category_encoders import OneHotEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.linear_model import LinearRegression, Ridge  # noqa F401\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "wqet_grader.init(\"Project 2 Assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. Assign this list to the variable name files\n",
    "files = glob('data/buenos-aires-real-estate-*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your wrangle function in a for loop to create a list named frames. The list should the cleaned DataFrames created from the CSV filenames your collected in files\n",
    "frames = []\n",
    "for file in files:\n",
    "    df=wrangle(file)\n",
    "    frames.append(df)\n",
    "frames[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.concat to concatenate the items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True\n",
    "df = pd.concat(frames, ignore_index= True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify your wrangle function to create a new feature \"neighborhood\". You can find the neighborhood for each property in the \"place_with_parent_names\" column. For example, a property with the place name \"|Argentina|Capital Federal|Palermo|\" is located in the neighborhood is \"Palermo\". Also, your function should drop the \"place_with_parent_names\" column\n",
    "df['neighborhood']=df['place_with_parent_names'].str.split('|', expand=True)[3]\n",
    "df.drop(columns='place_with_parent_names', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying the Function\n",
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "    \n",
    "    #Extract Neighbourhood\n",
    "    df['neighborhood']=df['place_with_parent_names'].str.split('|', expand=True)[3]\n",
    "    df.drop(columns='place_with_parent_names', inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your feature matrix X_train and target vector y_train. X_train should contain one feature: \"neighborhood\". Your target is \"price_aprox_usd\"\n",
    "target = \"price_aprox_usd\"\n",
    "features = [\"neighborhood\"]\n",
    "y_train = df[target]\n",
    "X_train = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the baseline mean absolute error for your model\n",
    "y_mean = y_train.mean()\n",
    "y_pred_baseline = [y_mean]*len(y_train)\n",
    "print(\"Mean apt price:\", y_mean)\n",
    "\n",
    "print(\"Baseline MAE:\", mean_absolute_error(y_train, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, instantiate a OneHotEncoder named ohe. Make sure to set the use_cat_names argument to True. Next, fit your transformer to the feature matrix X_train. Finally, use your encoder to transform the feature matrix X_train, and assign the transformed data to the variable XT_train\n",
    "#Instantiate\n",
    "ohe = OneHotEncoder(use_cat_names=True)\n",
    "#Fit\n",
    "ohe.fit(X_train)\n",
    "#Transform\n",
    "XT_train =ohe.transform(X_train)\n",
    "print(XT_train.shape)\n",
    "XT_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline named model that contains a OneHotEncoder transformer and a LinearRegression predictor. Then fit your model to the training data\n",
    "model = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True), \n",
    "    LinearRegression()\n",
    ")\n",
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, create a list of predictions for the observations in your feature matrix X_train. Name this list y_pred_training. Then calculate the training mean absolute error for your predictions in y_pred_training as compared to the true targets in y_train\n",
    "y_pred_training = model.predict(X_train)\n",
    "mae_training = mean_absolute_error(y_train,y_pred_training)\n",
    "print(\"Training MAE:\", round(mae_training, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the code below to import your test data buenos-aires-test-features.csv into a DataFrame and generate a Series of predictions using your model. Then run the following cell to submit your predictions to the grader\n",
    "X_test = pd.read_csv(\"data/buenos-aires-test-features.csv\")[features]\n",
    "y_pred_test = pd.Series(model.predict(X_test))\n",
    "y_pred_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the intercept and coefficients for your model\n",
    "intercept = model.named_steps['linearregression'].intercept_\n",
    "coefficients = model.named_steps['linearregression'].coef_\n",
    "print(\"coefficients len:\", len(coefficients))\n",
    "print(coefficients[:5])  # First five coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the feature names of your encoded data from the OneHotEncoder in your model\n",
    "feature_names = model.named_steps['onehotencoder'].get_feature_names()\n",
    "print(\"features len:\", len(feature_names))\n",
    "print(feature_names[:5])  # First five feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pandas Series named feat_imp where the index is your features and the values are your coefficients\n",
    "feat_imp = pd.Series(coefficients, index=feature_names)\n",
    "feat_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the cell below to print the equation that your model has determined for predicting apartment price based on longitude and latitude\n",
    "print(f\"price = {intercept.round(2)}\")\n",
    "for f, c in feat_imp.items():\n",
    "    print(f\"+ ({round(c, 2)} * {f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are high dimensionalites in data which causes overfitting.In this situation I have to use ridge regresssion instead of Linear regression for regularization. I have to change the model from make pipeline till end steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline named model that contains a OneHotEncoder transformer and a LinearRegression predictor. Then fit your model to the training data\n",
    "model = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True), \n",
    "    Ridge()\n",
    ")\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, create a list of predictions for the observations in your feature matrix X_train. Name this list y_pred_training. Then calculate the training mean absolute error for your predictions in y_pred_training as compared to the true targets in y_train\n",
    "y_pred_training = model.predict(X_train)\n",
    "mae_training = mean_absolute_error(y_train,y_pred_training)\n",
    "print(\"Training MAE:\", round(mae_training, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"data/buenos-aires-test-features.csv\")[features]\n",
    "y_pred_test = pd.Series(model.predict(X_test))\n",
    "y_pred_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicate Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the intercept and coefficients for your model\n",
    "intercept = model.named_steps['ridge'].intercept_\n",
    "coefficients = model.named_steps['ridge'].coef_\n",
    "print(\"coefficients len:\", len(coefficients))\n",
    "print(coefficients[:5])  # First five coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the feature names of your encoded data from the OneHotEncoder in your model\n",
    "feature_names = model.named_steps['onehotencoder'].get_feature_names()\n",
    "print(\"features len:\", len(feature_names))\n",
    "print(feature_names[:5])  # First five feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pandas Series named feat_imp where the index is your features and the values are your coefficients\n",
    "feat_imp = pd.Series(coefficients, index=feature_names)\n",
    "feat_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the cell below to print the equation that your model has determined for predicting apartment price based on longitude and latitude\n",
    "print(f\"price = {intercept.round(2)}\")\n",
    "for f, c in feat_imp.items():\n",
    "    print(f\"+ ({round(c, 2)} * {f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp.sort_values(key=abs).tail(15).plot(kind='barh')\n",
    "plt.xlabel('Importance [USD]')\n",
    "plt.ylabel('features')\n",
    "plt.title('Features Importance for Apartment Price')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
