{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Price with Size, Location and Neighborhood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wqet_grader\n",
    "from category_encoders import OneHotEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from ipywidgets import Dropdown, FloatSlider, IntSlider, interact\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge  # noqa F401\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "wqet_grader.init(\"Project 2 Assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    # Get place name\n",
    "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. Assign this list to the variable name file\n",
    "files = glob('data/buenos-aires-real-estate-*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your wrangle function in a list comprehension to create a list named frames. The list should contain the cleaned DataFrames for the filenames your collected in files\n",
    "frames = [wrangle(file) for file in files]\n",
    "len(frames)\n",
    "type(frames[0])\n",
    "frames[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.concat to concatenate it items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify your wrangle function to drop any columns that are more than half NaN values. Be sure to rerun all the cells above before you continue\n",
    "df.drop(columns=['floor','expenses'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    # Get place name\n",
    "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
    "    \n",
    "    #drop features with high null count\n",
    "    df.drop(columns=['floor','expenses'], inplace=True)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. Assign this list to the variable name file\n",
    "files = glob('data/buenos-aires-real-estate-*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your wrangle function in a list comprehension to create a list named frames. The list should contain the cleaned DataFrames for the filenames your collected in files\n",
    "frames = [wrangle(file) for file in files]\n",
    "len(frames)\n",
    "type(frames[0])\n",
    "frames[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.concat to concatenate it items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of unique values for each non-numeric feature in df\n",
    "df.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify Function 2nd time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    # Get place name\n",
    "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
    "    \n",
    "    #drop features with high null count\n",
    "    df.drop(columns=['floor','expenses'], inplace=True)\n",
    "    \n",
    "    #drop high and low cardinality catagorical variables\n",
    "    df.drop(columns=['operation', 'property_type','currency','properati_url'], inplace=True)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. Assign this list to the variable name file\n",
    "files = glob('data/buenos-aires-real-estate-*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your wrangle function in a list comprehension to create a list named frames. The list should contain the cleaned DataFrames for the filenames your collected in files\n",
    "frames = [wrangle(file) for file in files]\n",
    "len(frames)\n",
    "type(frames[0])\n",
    "frames[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.concat to concatenate it items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(df.info())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    # Get place name\n",
    "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
    "    \n",
    "    #drop features with high null count\n",
    "    df.drop(columns=['floor','expenses'], inplace=True)\n",
    "    \n",
    "    #drop high and low cardinality catagorical variables\n",
    "    df.drop(columns=['operation', 'property_type','currency','properati_url'], inplace=True)\n",
    "    \n",
    "    #drop leaky columns\n",
    "    df.drop(columns=['price','price_aprox_local_currency', 'price_per_m2', 'price_usd_per_m2'], inplace=True)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. Assign this list to the variable name file\n",
    "files = glob('data/buenos-aires-real-estate-*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.concat to concatenate it items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(df.info())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a correlation heatmap of the remaining numerical features in df. Since \"price_aprox_usd\" will be your target, you don't need to include it in your heatmap\n",
    "corr = df.select_dtypes('number').drop(columns='price_aprox_usd').corr()\n",
    "sns.heatmap(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    # Get place name\n",
    "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
    "    \n",
    "    #drop features with high null count\n",
    "    df.drop(columns=['floor','expenses'], inplace=True)\n",
    "    \n",
    "    #drop high and low cardinality catagorical variables\n",
    "    df.drop(columns=['operation', 'property_type','currency','properati_url'], inplace=True)\n",
    "    \n",
    "    #drop leaky columns\n",
    "    df.drop(columns=['price','price_aprox_local_currency', 'price_per_m2', 'price_usd_per_m2'], inplace=True)\n",
    "    \n",
    "    #drop columns with multicollinearity\n",
    "    df.drop(columns=['surface_total_in_m2', 'rooms'], inplace=True)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. Assign this list to the variable name file\n",
    "files = glob('data/buenos-aires-real-estate-*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your wrangle function in a list comprehension to create a list named frames. The list should contain the cleaned DataFrames for the filenames your collected in files\n",
    "frames = [wrangle(file) for file in files]\n",
    "len(frames)\n",
    "type(frames[0])\n",
    "frames[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.concat to concatenate it items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(df.info())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.concat to concatenate it items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(df.info())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your feature matrix X_train and target vector y_train. Your target is \"price_aprox_usd\". Your features should be all the columns that remain in the DataFrame you cleaned above\n",
    "target = \"price_aprox_usd\"\n",
    "y_train=df[target]\n",
    "features=[\"surface_covered_in_m2\", \"lat\",\"lon\", \"neighborhood\"]\n",
    "X_train=df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the baseline mean absolute error for your model.\n",
    "y_mean=y_train.mean()\n",
    "y_pred_baseline=[y_mean]*len(y_train)\n",
    "print(\"Mean apt price:\", y_mean)\n",
    "\n",
    "print(\"Baseline MAE:\", mean_absolute_error(y_train,y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline named model that contains a OneHotEncoder, SimpleImputer, and Ridge predictor\n",
    "model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    SimpleImputer(),\n",
    "    Ridge()\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the training mean absolute error for your predictions as compared to the true targets in y_train\n",
    "y_pred_training=model.predict(X_train)\n",
    "print(\"Training MAE:\", mean_absolute_error(y_train, y_pred_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the code below to import your test data buenos-aires-test-features.csv into a DataFrame and generate a list of predictions using your model. Then run the following cell to submit your predictions to the grader.\n",
    "X_test = pd.read_csv(\"data/buenos-aires-test-features.csv\")\n",
    "y_pred_test = pd.Series(model.predict(X_test))\n",
    "y_pred_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function make_prediction that takes four arguments (area, lat, lon, and neighborhood) and returns your model's prediction for an apartment price\n",
    "def make_prediction(area, lat, lon, neighborhood):\n",
    "    data={\n",
    "        \"surface_covered_in_m2\":area,\n",
    "        \"lat\":lat,\n",
    "        \"lon\":lon,\n",
    "        \"neighborhood\":neighborhood\n",
    "    }\n",
    "    df=pd.DataFrame(data, index=[0])\n",
    "    prediction = model.predict(df).round(2)[0]\n",
    "    return f\"Predicted apartment price: ${prediction}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add your make_prediction to the interact widget below, run the cell, and then adjust the widget to see how predicted apartment price changes.\n",
    "interact(\n",
    "    make_prediction,\n",
    "    area=IntSlider(\n",
    "        min=X_train[\"surface_covered_in_m2\"].min(),\n",
    "        max=X_train[\"surface_covered_in_m2\"].max(),\n",
    "        value=X_train[\"surface_covered_in_m2\"].mean(),\n",
    "    ),\n",
    "    lat=FloatSlider(\n",
    "        min=X_train[\"lat\"].min(),\n",
    "        max=X_train[\"lat\"].max(),\n",
    "        step=0.01,\n",
    "        value=X_train[\"lat\"].mean(),\n",
    "    ),\n",
    "    lon=FloatSlider(\n",
    "        min=X_train[\"lon\"].min(),\n",
    "        max=X_train[\"lon\"].max(),\n",
    "        step=0.01,\n",
    "        value=X_train[\"lon\"].mean(),\n",
    "    ),\n",
    "    neighborhood=Dropdown(options=sorted(X_train[\"neighborhood\"].unique())),\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3863b2c5477f4dfe9734428f9b5659b9c57b4995a7858c229730ccb9c81899f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
